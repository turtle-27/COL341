{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import glob\nimport os\nimport sys\n\nimport torch\nimport torchvision\n\nimport pandas as pd\n\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils, models\n\nfrom skimage import io, transform\n\nfrom torch.optim import SGD\nfrom torch.nn import CrossEntropyLoss, Dropout, Flatten\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport PIL\nfrom PIL import Image as img\n\nfrom IPython.display import Image\n\nGLOBAL_LABEL = {}\nGLOBAL_LABEL_REV = {}\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, train = True, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(img_dir + annotations_file)\n        self.img_dir = img_dir\n        self.is_train = train\n        \n        if train == True:\n            self.img_labels.iloc[:,1] = self.img_labels.iloc[:,1].map(GLOBAL_LABEL)\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = PIL.Image.open(img_path)\n        \n        if self.is_train:\n            label = self.img_labels.iloc[idx, 1]\n        else:\n            label = -1\n\n        image = self.transform(image)\n            \n        if self.target_transform:\n            label = self.target_transform(label)\n            \n        sample = {\"image\": image, \"label\": label}\n        \n        return sample\n\ndef createGlobalDic(trainingFile):\n    df = pd.read_csv(trainingFile)\n\n    global GLOBAL_LABEL, GLOBAL_LABEL_REV\n\n    categ = df['category']\n    asanas = categ.unique()\n\n    for index,value in enumerate(asanas):\n        GLOBAL_LABEL_REV[index] = value \n        GLOBAL_LABEL[value] = index\n\ndef loadData(file):\n    BATCH_SIZE = 50 \n    NUM_WORKERS = 20\n\n    stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n#     img_tran = transforms.Compose([transforms.Resize((224,224)),\n#                                     transforms.RandomHorizontalFlip(),\n#                                     transforms.ToTensor(), \n#                                     transforms.Normalize(*stats,inplace=True)])\n\n    img_tran = transforms.Compose([transforms.Resize(299),\n                                    transforms.RandomHorizontalFlip(),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                ])\n\n    root = file\n\n    training_data = CustomImageDataset(annotations_file = \"training.csv\", img_dir = root,transform=img_tran)\n    train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)\n\n    return train_dataloader\n\ndef modelLoader(train_dataloader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    criterion = CrossEntropyLoss()\n\n#     model = models.googlenet(pretrained=True)\n#     model = torchvision.models.mnasnet1_3(pretrained = False)\n\n    model = models.inception_v3(pretrained = True)\n    \n    # pytorch_total_params = sum(p.numel() for p in model.parameters())\n    # print(pytorch_total_params)\n    # print(model)\n\n    model.fc = nn.Sequential( nn.Linear(model.fc.in_features, 19),)\n                       \n    optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=4e-5, nesterov = True)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer,  max_lr = 0.1, epochs = 40, steps_per_epoch = len(train_dataloader))\n   \n    if not torch.cuda.is_available():\n        return [model, criterion, optimizer, scheduler]\n\n    model = model.cuda()\n    criterion = criterion.cuda()\n\n    return [model, criterion, optimizer, scheduler]\n\ndef main():\n\n    trainingFile = \"../input/col341-a3/\"\n    modelFile = \"./model.pth\"\n\n    createGlobalDic(trainingFile+\"training.csv\")\n\n    train_dataloader = loadData(trainingFile)\n\n    model, criterion, optimizer, scheduler = modelLoader(train_dataloader)\n\n    epochs = 10\n    steps = 0\n    running_loss = 0\n    print_every = 10\n    train_losses, test_losses = [], []\n    \n    test_csv = \"test.csv\"\n    stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    valid_tfms = transforms.Compose([transforms.Resize(299),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                    ])\n\n    BATCH_SIZE = 50 \n    NUM_WORKERS = 20\n\n    root = \"../input/col341-a3/\"\n\n    test_dataset = CustomImageDataset(annotations_file = test_csv, train=False, img_dir = root, transform=valid_tfms)\n    test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)\n\n    for epoch in range(epochs):\n\n        print(\"Epoch: \", epoch)\n        count = 0\n\n        for batch_idx, sample in enumerate(train_dataloader):\n            steps += 1\n            \n            if not torch.cuda.is_available():\n                inputs = sample['image']\n                labels = sample['label']\n            else:\n                inputs = sample['image'].cuda()\n                labels = sample['label'].cuda()\n                \n            logps = model.forward(inputs).logits\n            loss = criterion(logps, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            running_loss += loss.item()\n\n            optimizer.step()\n            scheduler.step()\n\n            model.train()\n\n            count += 1\n\n        print('Epoch : ',epoch+1, '\\t', 'loss :', running_loss/count)\n        running_loss = 0\n\n        train_losses.append(running_loss/count)\n        \n        if (epoch > 10) :\n            torch.save(model.state_dict(), modelFile)\n            \n            modelFile = \"./model.pth\"\n            testFile = \"../input/col341-a3/test.csv\"\n            outputFile = \"./submission.csv\"\n\n            model1, criterion1, optimizer1, scheduler1 = modelLoader(test_loader)\n            model1.load_state_dict(torch.load(modelFile))\n            model1.eval()\n\n            total_count = 0\n            count = 0\n\n            predictions = []\n\n            for batch_idx, sample in enumerate(test_loader):\n\n                if not torch.cuda.is_available():\n                    x_test = sample['image']\n                    y_test = sample['label'] \n                else:\n                    x_test = sample['image'].cuda()\n                    y_test = sample['label'].cuda()\n\n                pred = model1(x_test)\n                pred = torch.argmax(pred, dim = 1)\n\n                for i in range(len(pred)):\n                    predictions.append(pred[i].item())\n\n            f = open(testFile, 'r')\n            filenames = f.readlines()\n            f.close()\n\n            print(len(predictions), len(filenames))\n\n            f = open(outputFile, 'w')\n            f.write('name,'+'category'+'\\n')\n\n            for i in range(len(predictions) - 1):\n                f.write(filenames[i+1][:-1] + ',' + GLOBAL_LABEL_REV[predictions[i]] + '\\n')\n\n            f.close()\n        \n    torch.save(model.state_dict(), modelFile)\n\nmain()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-10T12:37:34.282864Z","iopub.execute_input":"2021-11-10T12:37:34.283265Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/104M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee689311888a4c428f790ebd29a376c6"}},"metadata":{}},{"name":"stdout","text":"Epoch:  0\nEpoch :  1 \t loss : 0.6981421775102821\n8800 8801\nEpoch:  1\nEpoch :  2 \t loss : 0.3423359273594649\nEpoch:  2\nEpoch :  3 \t loss : 0.3283222024577466\n8800 8801\nEpoch:  3\nEpoch :  4 \t loss : 0.30669325532747527\nEpoch:  4\nEpoch :  5 \t loss : 0.2843085771615708\n8800 8801\nEpoch:  5\nEpoch :  6 \t loss : 0.25469143149882184\nEpoch:  6\nEpoch :  7 \t loss : 0.21963468751043166\n8800 8801\nEpoch:  7\nEpoch :  8 \t loss : 0.19305715337295482\nEpoch:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working/')\n\nfrom IPython.display import FileLink\nFileLink(r'model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_csv = \"test.csv\"\nstats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n# valid_tfms = transforms.Compose([transforms.Resize((224,224)),\n#                          transforms.ToTensor(), \n#                          transforms.Normalize(*stats,inplace=True)])\n\nvalid_tfms = transforms.Compose([transforms.Resize(299),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                ])\n\n# training_data = CustomImageDataset(annotations_file = train_csv, img_dir = root,transform=img_tran)\n# train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)\n\nBATCH_SIZE = 50 \nNUM_WORKERS = 20\n\nroot = \"../input/col341-a3/\"\n\ntest_dataset = CustomImageDataset(annotations_file = test_csv, train=False, img_dir = root, transform=valid_tfms)\ntest_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelFile = \"./model.pth\"\ntestFile = \"../input/col341-a3/test.csv\"\noutputFile = \"./submission.csv\"\n    \nmodel, criterion, optimizer, scheduler = modelLoader(test_loader)\nmodel.load_state_dict(torch.load(modelFile))\nmodel.eval()\n\ntotal_count = 0\ncount = 0\n\npredictions = []\n\nfor batch_idx, sample in enumerate(test_loader):\n\n    if not torch.cuda.is_available():\n        x_test = sample['image']\n        y_test = sample['label'] \n    else:\n        x_test = sample['image'].cuda()\n        y_test = sample['label'].cuda()\n\n    pred = model(x_test)\n    pred = torch.argmax(pred, dim = 1)\n\n    for i in range(len(pred)):\n        predictions.append(pred[i].item())\n\nf = open(testFile, 'r')\nfilenames = f.readlines()\nf.close()\n\nprint(len(predictions), len(filenames))\n\nf = open(outputFile, 'w')\nf.write('name,'+'category'+'\\n')\n\nfor i in range(len(predictions) - 1):\n    f.write(filenames[i+1][:-1] + ',' + GLOBAL_LABEL_REV[predictions[i]] + '\\n')\n\nf.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}